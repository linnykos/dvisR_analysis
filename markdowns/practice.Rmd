---
title: "Research_Practice"
output: 
  html_document:
    toc:  true
    toc_float:  true
    code_folding:  show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This documnet is a practice analysis on Ziesel dataset

# Data Import

## Import dataset

```{r}
# Import Ziesel dataset
dat <- read.csv("Zeisel_preprocessed.csv", row.names = 1)
cell_type <- read.table("Zeisel_cell_info.txt", sep = "\t", header = 1)

# Get the labels for each cell
cluster_labels <- as.numeric(as.factor(cell_type$level1class))
```

# Dependence Measures

## 1.	Pearson’s correlation coefficient

- it measures the linear dependence.
- the runtime is very short compared to other methods.

```{r}
cor_pearson_mat <- stats::cor(dat, method = "pearson")

cor_pearson_mat[upper.tri(cor_pearson_mat, diag = T)] <- NA
cor_pearson_mat[1:5,1:5]
```

```{r}
# plot the smallest correlations
cor_pearson_vec <- sort(abs(cor_pearson_mat), decreasing = T)
plot(cor_pearson_vec)
```

```{r}
#plot the high correlations
par(mfrow = c(2,2))
for(i in 1:4){
 idx <- which(abs(cor_pearson_mat) == cor_pearson_vec[i], arr.ind = T)
 idx1 <- idx[1]; idx2 <- idx[2]
 
 plot(dat[,idx1], dat[,idx2], col = cluster_labels, asp = T,
      pch = 16, xlab = paste0(colnames(dat)[idx1], ", (", idx1, ")"),
      ylab = paste0(colnames(dat)[idx2], ", (", idx2, ")"), 
      main = paste0("Correlation of ", round(cor_pearson_mat[idx1, idx2], 3)))
}
```

```{r}
#plot the lowest correlations
par(mfrow = c(2,2))
for(i in 1:4){
 idx <- which(abs(cor_pearson_mat) == rev(cor_pearson_vec)[i], arr.ind = T)
 idx1 <- idx[1]; idx2 <- idx[2]
 
 plot(dat[,idx1], dat[,idx2], col = cluster_labels, asp = T,
      pch = 16, xlab = paste0(colnames(dat)[idx1], ", (", idx1, ")"),
      ylab = paste0(colnames(dat)[idx2], ", (", idx2, ")"), 
      main = paste0("Correlation of ", round(cor_pearson_mat[idx1, idx2], 3)))
}
```

## 2.	Spearman’s correlation coefficient

- captures monotonous relationship within data.
- the runtime is very short compared to other methods.

```{r}
cor_spearman_mat <- stats::cor(dat, method = "spearman")

cor_spearman_mat[upper.tri(cor_spearman_mat, diag = T)] <- NA
cor_spearman_mat[1:5,1:5]
```


```{r}
# plot the smallest correlations
cor_spearman_vec <- sort(abs(cor_spearman_mat), decreasing = T)
plot(cor_spearman_vec)
```

```{r}
#plot the high correlations
par(mfrow = c(2,2))
for(i in 1:4){
 idx <- which(abs(cor_spearman_mat) == cor_spearman_vec[i], arr.ind = T)
 idx1 <- idx[1]; idx2 <- idx[2]
 
 plot(dat[,idx1], dat[,idx2], col = cluster_labels, asp = T,
      pch = 16, xlab = paste0(colnames(dat)[idx1], ", (", idx1, ")"),
      ylab = paste0(colnames(dat)[idx2], ", (", idx2, ")"), 
      main = paste0("Correlation of ", round(cor_spearman_mat[idx1, idx2], 3)))
}
```

```{r}
#plot the lowest correlations
par(mfrow = c(2,2))
for(i in 1:4){
 idx <- which(abs(cor_spearman_mat) == rev(cor_spearman_vec)[i], arr.ind = T)
 idx1 <- idx[1]; idx2 <- idx[2]
 
 plot(dat[,idx1], dat[,idx2], col = cluster_labels, asp = T,
      pch = 16, xlab = paste0(colnames(dat)[idx1], ", (", idx1, ")"),
      ylab = paste0(colnames(dat)[idx2], ", (", idx2, ")"), 
      main = paste0("Correlation of ", round(cor_spearman_mat[idx1, idx2], 3)))
}
```

## 3.	Kendall’s correlation coefficient, τ

- alternative method to Spearman’s correlations, identifying monotonic relationships.
- it takes more time than Pearson and Spearman do, so I decide to apply this method on the subset of dataset of which the size is 300 rows. 

```{r}
cor_kendall_mat <- stats::cor(dat[1:300,], method = "kendall")

cor_kendall_mat[upper.tri(cor_kendall_mat, diag = T)] <- NA
cor_kendall_mat[1:5,1:5]
```

```{r}
# plot the smallest correlations
cor_kendall_vec <- sort(abs(cor_kendall_mat), decreasing = T)
plot(cor_kendall_vec)
```

```{r}
#plot the high correlations
par(mfrow = c(2,2))
for(i in 1:4){
 idx <- which(abs(cor_kendall_mat) == cor_kendall_vec[i], arr.ind = T)
 idx1 <- idx[1]; idx2 <- idx[2]
 
 plot(dat[,idx1], dat[,idx2], col = cluster_labels, asp = T,
      pch = 16, xlab = paste0(colnames(dat)[idx1], ", (", idx1, ")"),
      ylab = paste0(colnames(dat)[idx2], ", (", idx2, ")"), 
      main = paste0("Correlation of ", round(cor_kendall_mat[idx1, idx2], 3)))
}
```

```{r}
#plot the lowest correlations
par(mfrow = c(2,2))
for(i in 1:4){
 idx <- which(abs(cor_kendall_mat) == rev(cor_kendall_vec)[i], arr.ind = T)
 idx1 <- idx[1]; idx2 <- idx[2]
 
 plot(dat[,idx1], dat[,idx2], col = cluster_labels, asp = T,
      pch = 16, xlab = paste0(colnames(dat)[idx1], ", (", idx1, ")"),
      ylab = paste0(colnames(dat)[idx2], ", (", idx2, ")"), 
      main = paste0("Correlation of ", round(cor_kendall_mat[idx1, idx2], 3)))
}
```

## 4.	Distance correlation

- it is a fully non-parametric measure that identifies non-linear relationships between two random variables with energy distances

```{r}
library(energy)

dist_cor <- dcor.test(as.numeric(dat[1,]), as.numeric(dat[2,]))
```

## 5. Hoeffding’s D measure

- tests the independence of the data sets by calculating the distance between the product of the marginal distributions

```{r}
library(Hmisc)

hoeffd_cor_mat <- hoeffd(x = as.matrix(dat[1:3

hoeff_dist <- hoeffd_cor_mat$D

#hoeff_dist

```

```{r}
# plot the smallest correlations
cor_hoeff_vec <- sort(abs(hoeff_dist), decreasing = T)
plot(cor_hoeff_vec)
```

```{r}
#plot the high correlations
par(mfrow = c(2,2))
for(i in 1:4){
 idx <- which(abs(hoeff_dist) == (cor_hoeff_vec)[i], arr.ind = T)
 idx1 <- idx[1]; idx2 <- idx[2]
 
 plot(dat[,idx1], dat[,idx2], col = cluster_labels, asp = T,
      pch = 16, xlab = paste0(colnames(dat)[idx1], ", (", idx1, ")"),
      ylab = paste0(colnames(dat)[idx2], ", (", idx2, ")"), 
      main = paste0("Correlation of ", round(hoeff_dist[idx1, idx2], 3)))
}
```

```{r}
#plot the lowest correlations
par(mfrow = c(2,2))
for(i in 1:4){
 idx <- which(abs(hoeff_dist) == rev(cor_hoeff_vec)[i], arr.ind = T)
 idx1 <- idx[1]; idx2 <- idx[2]
 
 plot(dat[,idx1], dat[,idx2], col = cluster_labels, asp = T,
      pch = 16, xlab = paste0(colnames(dat)[idx1], ", (", idx1, ")"),
      ylab = paste0(colnames(dat)[idx2], ", (", idx2, ")"), 
      main = paste0("Correlation of ", round(hoeff_dist[idx1, idx2], 3)))
}
```

## 6.	Mutual information (MI)

- measures how much one random variable tells us about another.

```{r}
library(entropy)
y2d <- discretize2d(as.matrix(dat[1,]), as.matrix(dat[3,]), numBins1 = 100, numBins2 = 100)
mi.empirical(y2d)
```

## 7. The Hilbert–Schmidt independence criterion (HSIC)

```{r}
library(dHSIC)

#dhsic(as.matrix(dat[1:1500,]), matrix.input=TRUE)
```